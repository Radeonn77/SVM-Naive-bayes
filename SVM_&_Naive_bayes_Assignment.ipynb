{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SVM & Naive bayes\n",
        "\n",
        "#1. What is a Support Vector Machine (SVM)?\n",
        "  - SVM is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that separates data points of different classes with the maximum margin. The points lying closest to this hyperplane are called support vectors, and they determine the position and orientation of the hyperplane.\n",
        "\n",
        "#2. Difference between Hard Margin and Soft Margin SVM\n",
        "- Hard Margin:\n",
        "\n",
        "   Assumes that data is perfectly linearly separable.\n",
        "\n",
        "    No misclassification allowed.\n",
        "\n",
        "    Can lead to overfitting in noisy datasets.\n",
        "\n",
        "- Soft Margin:\n",
        "\n",
        "Allows some misclassifications using a penalty parameter\n",
        "C.\n",
        "\n",
        "More robust to noise and overlapping classes.\n",
        "\n",
        "#3. Mathematical intuition behind SVM\n",
        "SVM maximizes the margin between classes by solving the optimization problem:\n",
        "\n",
        "min\n",
        "‚Å°\n",
        "ùë§\n",
        ",\n",
        "ùëè\n",
        "1\n",
        "2\n",
        "‚à£\n",
        "‚à£\n",
        "ùë§\n",
        "‚à£\n",
        "‚à£\n",
        "2\n",
        "w,b\n",
        "min\n",
        "‚Äã\n",
        "  \n",
        "2\n",
        "1\n",
        "‚Äã\n",
        " ‚à£‚à£w‚à£‚à£\n",
        "2\n",
        "\n",
        "subject to:\n",
        "\n",
        "ùë¶\n",
        "ùëñ\n",
        "(\n",
        "ùë§\n",
        "‚ãÖ\n",
        "ùë•\n",
        "ùëñ\n",
        "+\n",
        "ùëè\n",
        ")\n",
        "‚â•\n",
        "1\n",
        "y\n",
        "i\n",
        "‚Äã\n",
        " (w‚ãÖx\n",
        "i\n",
        "‚Äã\n",
        " +b)‚â•1\n",
        "for all training samples\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        ",\n",
        "ùë¶\n",
        "ùëñ\n",
        ")\n",
        "(x\n",
        "i\n",
        "‚Äã\n",
        " ,y\n",
        "i\n",
        "‚Äã\n",
        " ).\n",
        "The margin\n",
        "2\n",
        "‚à£\n",
        "‚à£\n",
        "ùë§\n",
        "‚à£\n",
        "‚à£\n",
        "‚à£‚à£w‚à£‚à£\n",
        "2\n",
        "‚Äã\n",
        "  is maximized to achieve better generalization.\n",
        "\n",
        "#4. Role of Lagrange Multipliers in SVM\n",
        "- SVM uses Lagrange multipliers in the optimization problem to handle constraints efficiently. They help in deriving the dual formulation of SVM, which makes it possible to use the kernel trick for non-linear classification.\n",
        "\n",
        "#5. What are Support Vectors in SVM?\n",
        "- Support vectors are the data points closest to the decision boundary (hyperplane). They are the most critical elements of the dataset because removing them changes the decision boundary, while removing other points usually does not.\n",
        "\n",
        "#6. What is a Support Vector Classifier (SVC)?\n",
        "- SVC is the classification variant of SVM, where the algorithm learns to separate classes by finding the optimal hyperplane using either linear or non-linear kernels.\n",
        "\n",
        "#7. What is a Support Vector Regressor (SVR)?\n",
        "- SVR is the regression version of SVM. Instead of class separation, SVR tries to fit a function within a margin of tolerance (\n",
        "ùúñ\n",
        "œµ) from the actual target values.\n",
        "\n",
        "#8. What is the Kernel Trick in SVM?\n",
        "- The kernel trick allows SVM to solve non-linear classification problems by mapping the original data into a higher-dimensional space using a kernel function (e.g., RBF, polynomial) without explicitly computing the transformation.\n",
        "\n",
        "#9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel\n",
        "- Linear Kernel: Best for linearly separable data. Fast computation.\n",
        "\n",
        "- Polynomial Kernel: Suitable when relationships between classes are polynomial. Can model more complex boundaries.\n",
        "\n",
        "- RBF Kernel: Most popular for non-linear problems. Uses Gaussian similarity measure to capture complex patterns.\n",
        "\n",
        "#10. Effect of the C parameter in SVM\n",
        "- High C: Less tolerance for misclassification ‚Üí risk of overfitting.\n",
        "\n",
        "- Low C: More tolerance for misclassification ‚Üí better generalization.\n",
        "\n",
        "#11. Role of the Gamma parameter in RBF Kernel SVM\n",
        "- High Gamma: Narrow decision boundary ‚Üí captures local patterns ‚Üí risk of overfitting.\n",
        "\n",
        "- Low Gamma: Wider decision boundary ‚Üí captures more global patterns ‚Üí may underfit.\n",
        "\n",
        "#12. What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "- Na√Øve Bayes is a probabilistic classifier based on Bayes‚Äô theorem with the assumption that all features are conditionally independent given the class label. It‚Äôs ‚Äúna√Øve‚Äù because in reality, features are often correlated.\n",
        "\n",
        "#13. What is Bayes‚Äô Theorem?\n",
        " - ùëÉ\n",
        "(\n",
        "ùê¥\n",
        "‚à£\n",
        "ùêµ\n",
        ")\n",
        "=\n",
        "ùëÉ\n",
        "(\n",
        "ùêµ\n",
        "‚à£\n",
        "ùê¥\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùê¥\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùêµ\n",
        ")\n",
        "P(A‚à£B)=\n",
        "P(B)\n",
        "P(B‚à£A)P(A)\n",
        "‚Äã\n",
        "\n",
        "It calculates the probability of event\n",
        "ùê¥\n",
        "A given\n",
        "ùêµ\n",
        "B, using prior probability\n",
        "ùëÉ\n",
        "(\n",
        "ùê¥\n",
        ")\n",
        "P(A), likelihood\n",
        "ùëÉ\n",
        "(\n",
        "ùêµ\n",
        "‚à£\n",
        "ùê¥\n",
        ")\n",
        "P(B‚à£A), and evidence\n",
        "ùëÉ\n",
        "(\n",
        "ùêµ\n",
        ")\n",
        "P(B).\n",
        "\n",
        "#14. Differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes\n",
        " - Gaussian NB: Assumes features follow a Gaussian distribution (continuous data).\n",
        "\n",
        " - Multinomial NB: Works with count data (e.g., word frequencies).\n",
        "\n",
        "- Bernoulli NB: Works with binary features (present/absent).\n",
        "\n",
        "#15. When should you use Gaussian Na√Øve Bayes over other variants?\n",
        "- When the features are continuous and follow a normal distribution, Gaussian NB is more appropriate.\n",
        "\n",
        "#16. Key assumptions made by Na√Øve Bayes\n",
        "- Features are conditionally independent given the class label.\n",
        "\n",
        "- All features contribute equally and independently to the outcome.\n",
        "\n",
        "#17. Advantages and disadvantages of Na√Øve Bayes\n",
        "- Advantages:\n",
        "\n",
        "   - Fast and efficient.\n",
        "\n",
        "  - Works well with high-dimensional data.\n",
        "\n",
        "  - Good for text classification.\n",
        "- Disadvantages:\n",
        "\n",
        "   - Strong independence assumption rarely holds.\n",
        "\n",
        "   - Poor performance if features are highly correlated.\n",
        "\n",
        "#18. Why is Na√Øve Bayes a good choice for text classification?\n",
        "- Handles high-dimensional sparse data well.\n",
        "\n",
        "- Works well with word count features (Multinomial NB).\n",
        "\n",
        "- Low training time.\n",
        "\n",
        "#19. Compare SVM and Na√Øve Bayes for classification tasks\n",
        "- SVM: More flexible, works well with complex boundaries, but slower.\n",
        "\n",
        "- Na√Øve Bayes: Faster, works well with text data, but limited by independence assumption.\n",
        "\n",
        "#20. How does Laplace Smoothing help in Na√Øve Bayes?\n",
        "- Laplace smoothing adds a small value (\n",
        "ùõº\n",
        "Œ±) to counts to avoid zero probability issues when a feature value hasn‚Äôt been seen in training.\n",
        "\n"
      ],
      "metadata": {
        "id": "JuXo0g-CaB8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Train an SVM Classifier on the Iris dataset\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris.data, iris.target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP97aJQGjetC",
        "outputId": "cb265f7b-4061-4453-a4c5-2b9b33b16b5a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Train Linear and RBF SVM on Wine dataset\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    wine.data, wine.target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Linear Kernel\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "print(\"Linear Kernel Accuracy:\", accuracy_score(y_test, linear_svm.predict(X_test)))\n",
        "\n",
        "# RBF Kernel\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "print(\"RBF Kernel Accuracy:\", accuracy_score(y_test, rbf_svm.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFgx_cYMlN8-",
        "outputId": "6d5832fc-13d2-435c-b221-d550c803158d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0\n",
            "RBF Kernel Accuracy: 0.8055555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Train Gaussian Na√Øve Bayes on Breast Cancer dataset\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    cancer.data, cancer.target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, nb_model.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7Qnb9qtlfF1",
        "outputId": "435443a7-0017-453a-f7d1-0fd5f2f6dfaa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Train Multinomial Na√Øve Bayes for text classification\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "data = fetch_20newsgroups(subset='train', categories=['rec.sport.baseball', 'sci.space'])\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(data.data)\n",
        "y = data.target\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X, y)\n",
        "print(\"Accuracy:\", model.score(X, y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQSyQ-6flrrk",
        "outputId": "5265797d-89a3-43a8-f0b9-746b75a3660c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9991596638655462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Compare SVM and Na√Øve Bayes on same dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris.data, iris.target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# SVM\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_acc = accuracy_score(y_test, svm_model.predict(X_test))\n",
        "\n",
        "# Na√Øve Bayes\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_acc = accuracy_score(y_test, nb_model.predict(X_test))\n",
        "\n",
        "print(\"SVM Accuracy:\", svm_acc)\n",
        "print(\"Na√Øve Bayes Accuracy:\", nb_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1f0YG54lzVr",
        "outputId": "7ab7c764-7bfa-4507-f7b9-b9640f154303"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 1.0\n",
            "Na√Øve Bayes Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Train an SVM Classifier with different C values and compare decision boundaries visually\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create dataset\n",
        "X, y = make_blobs(n_samples=50, centers=2, random_state=6)\n",
        "\n",
        "# Different C values\n",
        "C_values = [0.1, 1, 10]\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, C in enumerate(C_values, 1):\n",
        "    model = SVC(kernel='linear', C=C)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    plt.subplot(1, 3, i)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=30)\n",
        "\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    # Create grid\n",
        "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
        "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
        "    YY, XX = np.meshgrid(yy, xx)\n",
        "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "    Z = model.decision_function(xy).reshape(XX.shape)\n",
        "\n",
        "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.7,\n",
        "               linestyles=['--', '-', '--'])\n",
        "\n",
        "    plt.title(f\"C = {C}\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "CWpmGEI6mEce",
        "outputId": "246f9649-f825-47c3-b753-1ddce97cbe4e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2125711631.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Create grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxlim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0myy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mYY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeshgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Train Bernoulli Na√Øve Bayes for binary classification\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n",
        "                           n_redundant=0, n_classes=2, random_state=42)\n",
        "\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X > 0, y)  # Bernoulli works with binary features\n",
        "\n",
        "print(\"Accuracy:\", bnb.score(X > 0, y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uzkrM37mb94",
        "outputId": "16a0c75f-853b-498e-f489-f751eadaa34c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Apply feature scaling before training an SVM and compare results\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Without scaling\n",
        "svm_unscaled = SVC(kernel='rbf')\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "print(\"Without scaling accuracy:\", svm_unscaled.score(X_test, y_test))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svm_scaled = SVC(kernel='rbf')\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "print(\"With scaling accuracy:\", svm_scaled.score(X_test_scaled, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dnoIkXnmmNF",
        "outputId": "03bdd3dc-8ab2-4316-9e3b-d25d04227470"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without scaling accuracy: 1.0\n",
            "With scaling accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Gaussian Na√Øve Bayes before and after Laplace smoothing\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb = GaussianNB(var_smoothing=1e-9)\n",
        "gnb.fit(X_train, y_train)\n",
        "print(\"Default accuracy:\", gnb.score(X_test, y_test))\n",
        "\n",
        "# With Laplace smoothing (by increasing var_smoothing)\n",
        "gnb_smooth = GaussianNB(var_smoothing=1e-6)\n",
        "gnb_smooth.fit(X_train, y_train)\n",
        "print(\"With smoothing accuracy:\", gnb_smooth.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgcXshlGmmar",
        "outputId": "abd5ee59-954b-48d7-d4a0-c131ac98e14e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default accuracy: 1.0\n",
            "With smoothing accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. SVM Classifier with GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': [0.001, 0.01, 0.1],\n",
        "    'kernel': ['rbf', 'linear']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, cv=5)\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Score:\", grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFxuLRDZmmn9",
        "outputId": "01a040e2-72b6-493b-d080-87f0452ed0c5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}\n",
            "Best Score: 0.9583333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Train an SVM Classifier on an imbalanced dataset with class weighting\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Create imbalanced dataset\n",
        "X_imb, y_imb = make_classification(n_samples=1000, n_features=10, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Without class weight\n",
        "svm_normal = SVC(kernel='linear')\n",
        "svm_normal.fit(X_imb, y_imb)\n",
        "print(\"Without class weight:\\n\", classification_report(y_imb, svm_normal.predict(X_imb)))\n",
        "\n",
        "# With class weight\n",
        "svm_weighted = SVC(kernel='linear', class_weight='balanced')\n",
        "svm_weighted.fit(X_imb, y_imb)\n",
        "print(\"With class weight:\\n\", classification_report(y_imb, svm_weighted.predict(X_imb)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj5kDT0DnBlY",
        "outputId": "4391a9f5-00f6-41f7-c33a-763a193e533d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without class weight:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.96       897\n",
            "           1       0.75      0.55      0.64       103\n",
            "\n",
            "    accuracy                           0.94      1000\n",
            "   macro avg       0.85      0.77      0.80      1000\n",
            "weighted avg       0.93      0.94      0.93      1000\n",
            "\n",
            "With class weight:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.91      0.94       897\n",
            "           1       0.51      0.86      0.64       103\n",
            "\n",
            "    accuracy                           0.90      1000\n",
            "   macro avg       0.75      0.89      0.79      1000\n",
            "weighted avg       0.93      0.90      0.91      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Na√Øve Bayes classifier for spam detection\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Example dataset\n",
        "data = pd.DataFrame({\n",
        "    'text': [\"Win money now\", \"Hello friend\", \"Get free prize\", \"Let's meet tomorrow\", \"Free tickets for you\"],\n",
        "    'label': [1, 0, 1, 0, 1]\n",
        "})\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(data['text'])\n",
        "y = data['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdRfioyxnB9V",
        "outputId": "d42ce4da-a5a3-40fe-d353-85e032b11255"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Train SVM & Na√Øve Bayes on same dataset and compare accuracy\n",
        "# Using Iris dataset\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "print(\"SVM Accuracy:\", svm_model.score(X_test, y_test))\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "print(\"Na√Øve Bayes Accuracy:\", nb_model.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "xV8MZaIpnCD4",
        "outputId": "ca37a027-05bf-4877-ee00-e60fb27b4b9c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Sparse data was passed for X, but dense data is required. Use '.toarray()' to convert to a dense numpy array.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1020863211.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Na√Øve Bayes Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \"\"\"\n\u001b[1;32m    265\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         return self._partial_fit(\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_refit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_partial_fit_first_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_call\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         array = _ensure_sparse_format(\n\u001b[0m\u001b[1;32m   1015\u001b[0m             \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(sparse_container, accept_sparse, dtype, copy, ensure_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mpadded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" for \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_name\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_name\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    614\u001b[0m             \u001b[0;34mf\"Sparse data was passed{padded_input}, but dense data is required. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0;34m\"Use '.toarray()' to convert to a dense numpy array.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Sparse data was passed for X, but dense data is required. Use '.toarray()' to convert to a dense numpy array."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Perform feature selection before Na√Øve Bayes\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Select top 2 features\n",
        "selector = SelectKBest(chi2, k=2)\n",
        "X_new = selector.fit_transform(X_train, y_train)\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_new, y_train)\n",
        "\n",
        "X_test_new = selector.transform(X_test)\n",
        "print(\"Accuracy with feature selection:\", nb_model.score(X_test_new, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "VPOULB7enCJC",
        "outputId": "717ae958-c77e-41b6-b349-5b24d5faf0ac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Sparse data was passed for X, but dense data is required. Use '.toarray()' to convert to a dense numpy array.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3536101834.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mX_test_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \"\"\"\n\u001b[1;32m    265\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         return self._partial_fit(\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_refit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_partial_fit_first_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_call\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         array = _ensure_sparse_format(\n\u001b[0m\u001b[1;32m   1015\u001b[0m             \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(sparse_container, accept_sparse, dtype, copy, ensure_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mpadded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" for \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_name\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_name\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    614\u001b[0m             \u001b[0;34mf\"Sparse data was passed{padded_input}, but dense data is required. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0;34m\"Use '.toarray()' to convert to a dense numpy array.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Sparse data was passed for X, but dense data is required. Use '.toarray()' to convert to a dense numpy array."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15. SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "\n",
        "ovr_svm = OneVsRestClassifier(SVC(kernel='linear'))\n",
        "ovo_svm = OneVsOneClassifier(SVC(kernel='linear'))\n",
        "\n",
        "ovr_svm.fit(X_train, y_train)\n",
        "ovo_svm.fit(X_train, y_train)\n",
        "\n",
        "print(\"OvR Accuracy:\", ovr_svm.score(X_test, y_test))\n",
        "print(\"OvO Accuracy:\", ovo_svm.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tyiNGrpnlHc",
        "outputId": "6beb96a4-ecb5-4401-f6b3-cb1485c6f147"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvR Accuracy: 0.0\n",
            "OvO Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16. SVM Classifier with Linear, Polynomial, and RBF kernels\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "for k in kernels:\n",
        "    model = SVC(kernel=k)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"{k} kernel accuracy:\", model.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfUviExinlKL",
        "outputId": "e9e0a993-8cec-4e7b-f928-7fcb5d8bb280"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear kernel accuracy: 0.0\n",
            "poly kernel accuracy: 0.0\n",
            "rbf kernel accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17. SVM with Stratified K-Fold Cross-Validation\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(SVC(kernel='linear'), X_train, y_train, cv=skf)\n",
        "print(\"Average accuracy:\", scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "A260wadjnlM7",
        "outputId": "35e732ce-6fea-4020-dc49-a1af40046704"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=4.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-758636220.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mskf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    412\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    413\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m             \u001b[0;31m# Sequentially call the tasks and yield the results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# pre_dispatch and n_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         iterable_with_config = (\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_with_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    412\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    413\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    405\u001b[0m                 (\n\u001b[1;32m    406\u001b[0m                     \u001b[0;34m\"Cannot have number of splits n_splits={0} greater\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=4."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Na√Øve Bayes with different prior probabilities\n",
        "priors = [None, [0.3, 0.7]]\n",
        "for p in priors:\n",
        "    model = GaussianNB(priors=p)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"Priors={p}, Accuracy={model.score(X_test, y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "mj0A3rKNnlP-",
        "outputId": "26750ea8-fd42-4bef-810d-1acccfb9441e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Sparse data was passed for X, but dense data is required. Use '.toarray()' to convert to a dense numpy array.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-29071576.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpriors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Priors={p}, Accuracy={model.score(X_test, y_test)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \"\"\"\n\u001b[1;32m    265\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         return self._partial_fit(\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_refit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_partial_fit_first_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_call\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         array = _ensure_sparse_format(\n\u001b[0m\u001b[1;32m   1015\u001b[0m             \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(sparse_container, accept_sparse, dtype, copy, ensure_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mpadded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" for \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_name\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_name\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    614\u001b[0m             \u001b[0;34mf\"Sparse data was passed{padded_input}, but dense data is required. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0;34m\"Use '.toarray()' to convert to a dense numpy array.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Sparse data was passed for X, but dense data is required. Use '.toarray()' to convert to a dense numpy array."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Recursive Feature Elimination (RFE) before SVM\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "rfe = RFE(SVC(kernel='linear'), n_features_to_select=2)\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "print(\"Selected features:\", rfe.support_)\n",
        "print(\"Ranking:\", rfe.ranking_)\n",
        "print(\"Accuracy:\", rfe.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySI6wtNOn_8v",
        "outputId": "2e21ec33-0046-44ff-f8df-be2a31025e1e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: [False False False False False False  True False False False False  True\n",
            " False False]\n",
            "Ranking: [11  4 13  9 12  2  1  8  5  6 10  1  3  7]\n",
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20. SVM Classifier evaluated with Precision, Recall, and F1-Score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YR6Xq7roAAP",
        "outputId": "e08ac11a-e0d8-4785-f86b-ecb9162a14fd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}